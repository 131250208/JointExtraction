{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "from pprint import pprint\n",
    "import unicodedata\n",
    "from transformers import AutoModel, BasicTokenizer, BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import time\n",
    "from common.utils import Preprocessor\n",
    "from tplinker import (HandshakingTaggingScheme, \n",
    "                      LayerNorm, \n",
    "                      DataMaker4Bert, \n",
    "                      DataMaker4BiLSTM, \n",
    "                      TPLinkerBert, \n",
    "                      TPLinkerBiLSTM,\n",
    "                      MetricsCalculator)\n",
    "import wandb\n",
    "import yaml\n",
    "from glove import Glove\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "config = yaml.load(open(\"eval_config.yaml\", \"r\"), Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"device_num\"])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = os.path.join(*config[\"test_data_dir\"])\n",
    "\n",
    "test_data_path_dict = {}\n",
    "for path, folds, files in os.walk(test_data_dir):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        file_name = re.match(\"(.*?)\\.json\", file_name).group(1)\n",
    "        test_data_path_dict[file_name] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config[\"test_batch_size\"]\n",
    "\n",
    "# for reproductivity\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = {}\n",
    "for file_name, path in test_data_path_dict.items():\n",
    "    test_data_dict[file_name] = json.load(open(path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    tokenize = lambda text: text.split(\" \")\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = text.split(\" \")\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func = tokenize, \n",
    "                            get_tok2char_span_map_func = get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate the max token number: 100%|██████████| 5000/5000 [00:02<00:00, 1728.49it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "for data in list(test_data_dict.values()):\n",
    "    all_data.extend(data)\n",
    "    \n",
    "max_tok_num = 0\n",
    "for sample in tqdm(all_data, desc = \"Calculate the max token number\"):\n",
    "    tokens = tokenize(sample[\"text\"])\n",
    "    max_tok_num = max(len(tokens), max_tok_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_tok_num: 137, lagger than max_test_seq_len: 100, test data will be split!\n",
      "force to split the test dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting into subtexts: 100%|██████████| 5000/5000 [00:06<00:00, 719.97it/s]\n"
     ]
    }
   ],
   "source": [
    "split_test_data = False\n",
    "if max_tok_num > config[\"max_test_seq_len\"]:\n",
    "    split_test_data = True\n",
    "    print(\"max_tok_num: {}, lagger than max_test_seq_len: {}, test data will be split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "else:\n",
    "    print(\"max_tok_num: {}, less than or equal to max_test_seq_len: {}, no need to split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "max_seq_len = min(max_tok_num, config[\"max_test_seq_len\"]) \n",
    "\n",
    "if config[\"force_split\"]:\n",
    "    split_test_data = True\n",
    "    print(\"force to split the test dataset!\")    \n",
    "\n",
    "ori_test_data_dict = copy.deepcopy(test_data_dict)\n",
    "if split_test_data:\n",
    "    test_data_dict = {}\n",
    "    for file_name, data in ori_test_data_dict.items():\n",
    "        test_data_dict[file_name] = preprocessor.split_into_short_samples(data, \n",
    "                                                                          max_seq_len, \n",
    "                                                                          sliding_len = config[\"sliding_len\"], \n",
    "                                                                          encoder = config[\"encoder\"], \n",
    "                                                                          data_type = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder(Tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id_path = os.path.join(*config[\"rel2id_path\"])\n",
    "rel2id = json.load(open(rel2id_path, \"r\", encoding = \"utf-8\"))\n",
    "handshaking_tagger = HandshakingTaggingScheme(rel2id = rel2id, max_seq_len = max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    data_maker = DataMaker4Bert(tokenizer, handshaking_tagger)\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "\n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    token2idx_path = os.path.join(*config[\"token2idx_path\"])\n",
    "    token2idx = json.load(open(token2idx_path, \"r\", encoding = \"utf-8\"))\n",
    "    idx2token = {idx:tok for tok, idx in token2idx.items()}\n",
    "    def text2indices(text, max_seq_len):\n",
    "        input_ids = []\n",
    "        tokens = text.split(\" \")\n",
    "        for tok in tokens:\n",
    "            if tok not in token2idx:\n",
    "                input_ids.append(token2idx['<UNK>'])\n",
    "            else:\n",
    "                input_ids.append(token2idx[tok])\n",
    "        if len(input_ids) < max_seq_len:\n",
    "            input_ids.extend([token2idx['<PAD>']] * (max_seq_len - len(input_ids)))\n",
    "        input_ids = torch.tensor(input_ids[:max_seq_len])\n",
    "        return input_ids\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = text.split(\" \")\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span\n",
    "    data_maker = DataMaker4BiLSTM(text2indices, get_tok2char_span_map, handshaking_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    roberta = AutoModel.from_pretrained(config[\"bert_path\"])\n",
    "    hidden_size = roberta.config.hidden_size\n",
    "    fake_inputs = torch.zeros([config[\"test_batch_size\"], max_seq_len, hidden_size])\n",
    "    rel_extractor = TPLinkerBert(roberta, \n",
    "                                 len(rel2id), \n",
    "                                 fake_inputs,\n",
    "                                 config[\"shaking_type\"],\n",
    "                                )\n",
    "    \n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    glove = Glove()\n",
    "    glove = glove.load(config[\"pretrained_word_embedding_path\"])\n",
    "    \n",
    "    # prepare embedding matrix\n",
    "    word_embedding_init_matrix = np.random.normal(-1, 1, size=(len(token2idx), config[\"word_embedding_dim\"]))\n",
    "    count_in = 0\n",
    "\n",
    "    # 在预训练词向量中的用该预训练向量\n",
    "    # 不在预训练集里的用随机向量\n",
    "    for ind, tok in tqdm(idx2token.items(), desc=\"Embedding matrix initializing...\"):\n",
    "        if tok in glove.dictionary:\n",
    "            count_in += 1\n",
    "            word_embedding_init_matrix[ind] = glove.word_vectors[glove.dictionary[tok]]\n",
    "\n",
    "    print(\"{:.4f} tokens are in the pretrain word embedding matrix\".format(count_in / len(idx2token))) # 命中预训练词向量的比例\n",
    "    word_embedding_init_matrix = torch.FloatTensor(word_embedding_init_matrix)\n",
    "    \n",
    "    fake_inputs = torch.zeros([config[\"test_batch_size\"], max_seq_len, config[\"dec_hidden_size\"]])\n",
    "    rel_extractor = TPLinkerBiLSTM(word_embedding_init_matrix, \n",
    "                                   config[\"emb_dropout\"], \n",
    "                                   config[\"enc_hidden_size\"], \n",
    "                                   config[\"dec_hidden_size\"],\n",
    "                                   config[\"rnn_dropout\"],\n",
    "                                   len(rel2id), \n",
    "                                   fake_inputs,\n",
    "                                   config[\"shaking_type\"],\n",
    "                                  )\n",
    "    \n",
    "rel_extractor = rel_extractor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCalculator(handshaking_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------model state nyt_best_06152_a7667cb3.pt loaded ----------------\n"
     ]
    }
   ],
   "source": [
    "model_state_path = config[\"model_state_dict_path\"]\n",
    "rel_extractor.load_state_dict(torch.load(model_state_path))\n",
    "rel_extractor.eval()\n",
    "print(\"------------model state {} loaded ----------------\".format(model_state_path.split(\"/\")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(rel_list):\n",
    "    rel_memory_set = set()\n",
    "    filtered_rel_list = []\n",
    "    for rel in rel_list:\n",
    "        rel_memory = \"{}\\u2E80{}\\u2E80{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], \n",
    "                                                                 rel[\"subj_tok_span\"][1], \n",
    "                                                                 rel[\"predicate\"], \n",
    "                                                                 rel[\"obj_tok_span\"][0], \n",
    "                                                                 rel[\"obj_tok_span\"][1])\n",
    "        if rel_memory not in rel_memory_set:\n",
    "            filtered_rel_list.append(rel)\n",
    "            rel_memory_set.add(rel_memory)\n",
    "    return filtered_rel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, ori_test_data):\n",
    "    '''\n",
    "    test_data: if split, it would be samples with subtext\n",
    "    ori_test_data: the original data has not been split, used to get original text here\n",
    "    '''\n",
    "    indexed_test_data = data_maker.get_indexed_data(test_data, max_seq_len, data_type = \"test\") # fill up to max_seq_len\n",
    "    test_dataloader = DataLoader(MyDataset(indexed_test_data), \n",
    "                              batch_size = batch_size, \n",
    "                              shuffle = False, \n",
    "                              num_workers = 6,\n",
    "                              drop_last = False,\n",
    "                              collate_fn = lambda data_batch: data_maker.generate_batch(data_batch, data_type = \"test\"),\n",
    "                             )\n",
    "    \n",
    "    pred_sample_list = []\n",
    "    for batch_test_data in tqdm(test_dataloader, desc = \"Predicting\"):\n",
    "        if config[\"encoder\"] == \"BERT\":\n",
    "            sample_list, batch_input_ids, \\\n",
    "            batch_attention_mask, batch_token_type_ids, \\\n",
    "            tok2char_span_list, _, _, _ = batch_test_data\n",
    "\n",
    "            batch_input_ids, \\\n",
    "            batch_attention_mask, \\\n",
    "            batch_token_type_ids = (batch_input_ids.to(device), \n",
    "                                      batch_attention_mask.to(device), \n",
    "                                      batch_token_type_ids.to(device))\n",
    "\n",
    "        elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "            sample_list, batch_input_ids, tok2char_span_list, _, _, _ = batch_test_data\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if config[\"encoder\"] == \"BERT\":\n",
    "                batch_ent_shaking_outputs, \\\n",
    "                batch_head_rel_shaking_outputs, \\\n",
    "                batch_tail_rel_shaking_outputs = rel_extractor(batch_input_ids, \n",
    "                                                          batch_attention_mask, \n",
    "                                                          batch_token_type_ids, \n",
    "                                                         )\n",
    "            elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "                batch_ent_shaking_outputs, \\\n",
    "                batch_head_rel_shaking_outputs, \\\n",
    "                batch_tail_rel_shaking_outputs = rel_extractor(batch_input_ids)\n",
    "\n",
    "        batch_ent_shaking_tag, \\\n",
    "        batch_head_rel_shaking_tag, \\\n",
    "        batch_tail_rel_shaking_tag = torch.argmax(batch_ent_shaking_outputs, dim = -1), \\\n",
    "                                     torch.argmax(batch_head_rel_shaking_outputs, dim = -1), \\\n",
    "                                     torch.argmax(batch_tail_rel_shaking_outputs, dim = -1)\n",
    "\n",
    "        for ind in range(len(sample_list)):\n",
    "            gold_sample = sample_list[ind]\n",
    "            text = gold_sample[\"text\"]\n",
    "            text_id = gold_sample[\"id\"]\n",
    "            tok2char_span = tok2char_span_list[ind]\n",
    "            ent_shaking_tag, \\\n",
    "            head_rel_shaking_tag, \\\n",
    "            tail_rel_shaking_tag = batch_ent_shaking_tag[ind], \\\n",
    "                                    batch_head_rel_shaking_tag[ind], \\\n",
    "                                    batch_tail_rel_shaking_tag[ind]\n",
    "            tok_offset, char_offset = gold_sample[\"tok_offset\"], gold_sample[\"char_offset\"]\n",
    "            rel_list = handshaking_tagger.decode_rel_fr_shaking_tag(text, \n",
    "                                                                    ent_shaking_tag, \n",
    "                                                                    head_rel_shaking_tag, \n",
    "                                                                    tail_rel_shaking_tag, \n",
    "                                                                    tok2char_span, \n",
    "                                                                    tok_offset = tok_offset, char_offset = char_offset)\n",
    "            pred_sample_list.append({\n",
    "                \"text\": text,\n",
    "                \"id\": text_id,\n",
    "                \"relation_list\": rel_list,\n",
    "            })\n",
    "            \n",
    "    # merge\n",
    "    text_id2rel_list = {}\n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        if text_id not in text_id2rel_list:\n",
    "            text_id2rel_list[text_id] = sample[\"relation_list\"]\n",
    "        else:\n",
    "            text_id2rel_list[text_id].extend(sample[\"relation_list\"])\n",
    "\n",
    "    text_id2text = {sample[\"id\"]:sample[\"text\"] for sample in ori_test_data}\n",
    "    merged_pred_sample_list = []\n",
    "    for text_id, rel_list in text_id2rel_list.items():\n",
    "        merged_pred_sample_list.append({\n",
    "            \"id\": text_id,\n",
    "            \"text\": text_id2text[text_id],\n",
    "            \"relation_list\": filter_duplicates(rel_list),\n",
    "        })\n",
    "        \n",
    "    return merged_pred_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed pred data: 5063it [00:06, 768.17it/s]\n",
      "Predicting: 100%|██████████| 159/159 [01:01<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data: 4930 samples with pred relations\n"
     ]
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for file_name, short_data in test_data_dict.items():\n",
    "    ori_test_data = ori_test_data_dict[file_name]\n",
    "    pred_sample_list = predict(short_data, ori_test_data)\n",
    "    res_dict[file_name] = pred_sample_list\n",
    "    pred_res_num = len([s for s in pred_sample_list if len(s[\"relation_list\"]) > 0])\n",
    "    print(\"{}: {} samples with pred relations\".format(file_name, pred_res_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_prf(pred_sample_list, gold_test_data, pattern = \"only_head\"):\n",
    "    text_id2gold_n_pred = {}\n",
    "    for sample in gold_test_data:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id] = {\n",
    "            \"gold_relation_list\": sample[\"relation_list\"],\n",
    "        }\n",
    "    \n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id][\"pred_relation_list\"] = sample[\"relation_list\"]\n",
    "\n",
    "    correct_num, pred_num, gold_num = 0, 0, 0\n",
    "    for gold_n_pred in text_id2gold_n_pred.values():\n",
    "        gold_rel_list = gold_n_pred[\"gold_relation_list\"]\n",
    "        pred_rel_list = gold_n_pred[\"pred_relation_list\"] if \"pred_relation_list\" in gold_n_pred else []\n",
    "        if pattern == \"only_head\":\n",
    "            gold_rel_set = set([\"{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], rel[\"predicate\"], rel[\"obj_tok_span\"][0]) for rel in gold_rel_list])\n",
    "            pred_rel_set = set([\"{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], rel[\"predicate\"], rel[\"obj_tok_span\"][0]) for rel in pred_rel_list])\n",
    "        elif pattern == \"head_n_tail\":\n",
    "            gold_rel_set = set([\"{}\\u2E80{}\\u2E80{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], rel[\"subj_tok_span\"][1], rel[\"predicate\"], rel[\"obj_tok_span\"][0], rel[\"obj_tok_span\"][1]) for rel in gold_rel_list])\n",
    "            pred_rel_set = set([\"{}\\u2E80{}\\u2E80{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], rel[\"subj_tok_span\"][1], rel[\"predicate\"], rel[\"obj_tok_span\"][0], rel[\"obj_tok_span\"][1]) for rel in pred_rel_list])\n",
    "        elif pattern == \"whole_text\":\n",
    "            gold_rel_set = set([\"{}\\u2E80{}\\u2E80{}\".format(rel[\"subject\"], rel[\"predicate\"], rel[\"object\"]) for rel in gold_rel_list])\n",
    "            pred_rel_set = set([\"{}\\u2E80{}\\u2E80{}\".format(rel[\"subject\"], rel[\"predicate\"], rel[\"object\"]) for rel in pred_rel_list])\n",
    "            \n",
    "        for rel_str in pred_rel_set:\n",
    "            if rel_str in gold_rel_set:\n",
    "                correct_num += 1\n",
    "\n",
    "        pred_num += len(pred_rel_set)\n",
    "        gold_num += len(gold_rel_set)\n",
    "#     print((correct_num, pred_num, gold_num))\n",
    "    prf = metrics.get_prf_scores(correct_num, pred_num, gold_num)\n",
    "    return prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Results -----------------------\n",
      "{'test_data': (0.9112727272727162, 0.9258620689655058, 0.9185094684900041)}\n"
     ]
    }
   ],
   "source": [
    "only_head = True\n",
    "filename2scores = {}\n",
    "for file_name, pred_samples in res_dict.items():\n",
    "    gold_test_data = ori_test_data_dict[file_name]\n",
    "    prf = get_test_prf(pred_samples, gold_test_data, pattern = config[\"correct\"])\n",
    "    filename2scores[file_name] = prf\n",
    "print(\"---------------- Results -----------------------\")\n",
    "pprint(filename2scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
