{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "from pprint import pprint\n",
    "import unicodedata\n",
    "from transformers import AutoModel, BasicTokenizer, BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import time\n",
    "from common.utils import Preprocessor\n",
    "from tplinker import (HandshakingTaggingScheme,\n",
    "                      DataMaker4Bert, \n",
    "                      DataMaker4BiLSTM, \n",
    "                      TPLinkerBert, \n",
    "                      TPLinkerBiLSTM,\n",
    "                      MetricsCalculator)\n",
    "import wandb\n",
    "import yaml\n",
    "from glove import Glove\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "config = yaml.load(open(\"eval_config.yaml\", \"r\"), Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"device_num\"])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = config[\"data_home\"]\n",
    "experiment_name = config[\"exp_name\"]\n",
    "test_data_path = os.path.join(data_home, experiment_name, config[\"test_data\"])\n",
    "batch_size = config[\"batch_size\"]\n",
    "rel2id_path = os.path.join(data_home, experiment_name, config[\"rel2id\"])\n",
    "save_res_dir = os.path.join(config[\"save_res_dir\"], experiment_name)\n",
    "\n",
    "# for reproductivity\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path_dict = {}\n",
    "for file_path in glob.glob(test_data_path):\n",
    "    file_name = re.search(\"(.*?)\\.json\", file_path.split(\"/\")[-1]).group(1)\n",
    "    test_data_path_dict[file_name] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = {}\n",
    "for file_name, path in test_data_path_dict.items():\n",
    "    test_data_dict[file_name] = json.load(open(path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    tokenize = lambda text: text.split(\" \")\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = text.split(\" \")\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func = tokenize, \n",
    "                            get_tok2char_span_map_func = get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate the max token number: 100%|██████████| 15541/15541 [00:00<00:00, 171164.39it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "for data in list(test_data_dict.values()):\n",
    "    all_data.extend(data)\n",
    "    \n",
    "max_tok_num = 0\n",
    "for sample in tqdm(all_data, desc = \"Calculate the max token number\"):\n",
    "    tokens = tokenize(sample[\"text\"])\n",
    "    max_tok_num = max(len(tokens), max_tok_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_tok_num: 100, less than or equal to max_test_seq_len: 512, no need to split!\n"
     ]
    }
   ],
   "source": [
    "split_test_data = False\n",
    "if max_tok_num > config[\"max_test_seq_len\"]:\n",
    "    split_test_data = True\n",
    "    print(\"max_tok_num: {}, lagger than max_test_seq_len: {}, test data will be split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "else:\n",
    "    print(\"max_tok_num: {}, less than or equal to max_test_seq_len: {}, no need to split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "max_seq_len = min(max_tok_num, config[\"max_test_seq_len\"]) \n",
    "\n",
    "if config[\"force_split\"]:\n",
    "    split_test_data = True\n",
    "    print(\"force to split the test dataset!\")    \n",
    "\n",
    "ori_test_data_dict = copy.deepcopy(test_data_dict)\n",
    "if split_test_data:\n",
    "    test_data_dict = {}\n",
    "    for file_name, data in ori_test_data_dict.items():\n",
    "        test_data_dict[file_name] = preprocessor.split_into_short_samples(data, \n",
    "                                                                          max_seq_len, \n",
    "                                                                          sliding_len = config[\"sliding_len\"], \n",
    "                                                                          encoder = config[\"encoder\"], \n",
    "                                                                          data_type = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder(Tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id = json.load(open(rel2id_path, \"r\", encoding = \"utf-8\"))\n",
    "handshaking_tagger = HandshakingTaggingScheme(rel2id = rel2id, max_seq_len = max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    data_maker = DataMaker4Bert(tokenizer, handshaking_tagger)\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "\n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    token2idx_path = os.path.join(data_home, experiment_name, config[\"token2idx\"])\n",
    "    token2idx = json.load(open(token2idx_path, \"r\", encoding = \"utf-8\"))\n",
    "    idx2token = {idx:tok for tok, idx in token2idx.items()}\n",
    "    def text2indices(text, max_seq_len):\n",
    "        input_ids = []\n",
    "        tokens = text.split(\" \")\n",
    "        for tok in tokens:\n",
    "            if tok not in token2idx:\n",
    "                input_ids.append(token2idx['<UNK>'])\n",
    "            else:\n",
    "                input_ids.append(token2idx[tok])\n",
    "        if len(input_ids) < max_seq_len:\n",
    "            input_ids.extend([token2idx['<PAD>']] * (max_seq_len - len(input_ids)))\n",
    "        input_ids = torch.tensor(input_ids[:max_seq_len])\n",
    "        return input_ids\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = text.split(\" \")\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span\n",
    "    data_maker = DataMaker4BiLSTM(text2indices, get_tok2char_span_map, handshaking_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    roberta = AutoModel.from_pretrained(config[\"bert_path\"])\n",
    "    hidden_size = roberta.config.hidden_size\n",
    "    fake_inputs = torch.zeros([config[\"batch_size\"], max_seq_len, hidden_size])\n",
    "    rel_extractor = TPLinkerBert(roberta, \n",
    "                                 len(rel2id), \n",
    "                                 fake_inputs,\n",
    "                                 config[\"shaking_type\"],\n",
    "                                )\n",
    "    \n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    # random init embedding matrix\n",
    "    word_embedding_init_matrix = np.random.normal(-1, 1, size=(len(token2idx), config[\"word_embedding_dim\"]))\n",
    "    word_embedding_init_matrix = torch.FloatTensor(word_embedding_init_matrix)\n",
    "    \n",
    "    fake_inputs = torch.zeros([config[\"batch_size\"], max_seq_len, config[\"dec_hidden_size\"]])\n",
    "    rel_extractor = TPLinkerBiLSTM(word_embedding_init_matrix, \n",
    "                                   config[\"emb_dropout\"], \n",
    "                                   config[\"enc_hidden_size\"], \n",
    "                                   config[\"dec_hidden_size\"],\n",
    "                                   config[\"rnn_dropout\"],\n",
    "                                   len(rel2id), \n",
    "                                   fake_inputs,\n",
    "                                   config[\"shaking_type\"],\n",
    "                                  )\n",
    "    \n",
    "rel_extractor = rel_extractor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCalculator(handshaking_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model state paths\n",
    "model_state_dir = config[\"model_state_dict_dir\"]\n",
    "target_run_ids = set(config[\"run_ids\"])\n",
    "run_id2model_state_paths = {}\n",
    "for root, dirs, files in os.walk(model_state_dir):\n",
    "    for file_name in files:\n",
    "        run_id = root.split(\"-\")[-1]\n",
    "        if re.match(\".*model_state.*\\.pt\", file_name) and run_id in target_run_ids:\n",
    "            if run_id not in run_id2model_state_paths:\n",
    "                run_id2model_state_paths[run_id] = []\n",
    "            model_state_path = os.path.join(root, file_name)\n",
    "            run_id2model_state_paths[run_id].append(model_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_k_paths(path_list, k):\n",
    "    path_list = sorted(path_list, key = lambda x: int(re.search(\"(\\d+)\", x.split(\"/\")[-1]).group(1)))\n",
    "#     pprint(path_list)\n",
    "    return path_list[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only last k models\n",
    "k = config[\"last_k_model\"]\n",
    "for run_id, path_list in run_id2model_state_paths.items():\n",
    "    run_id2model_state_paths[run_id] = get_last_k_paths(path_list, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(rel_list):\n",
    "    rel_memory_set = set()\n",
    "    filtered_rel_list = []\n",
    "    for rel in rel_list:\n",
    "        rel_memory = \"{}\\u2E80{}\\u2E80{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], \n",
    "                                                                 rel[\"subj_tok_span\"][1], \n",
    "                                                                 rel[\"predicate\"], \n",
    "                                                                 rel[\"obj_tok_span\"][0], \n",
    "                                                                 rel[\"obj_tok_span\"][1])\n",
    "        if rel_memory not in rel_memory_set:\n",
    "            filtered_rel_list.append(rel)\n",
    "            rel_memory_set.add(rel_memory)\n",
    "    return filtered_rel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, ori_test_data):\n",
    "    '''\n",
    "    test_data: if split, it would be samples with subtext\n",
    "    ori_test_data: the original data has not been split, used to get original text here\n",
    "    '''\n",
    "    indexed_test_data = data_maker.get_indexed_data(test_data, max_seq_len, data_type = \"test\") # fill up to max_seq_len\n",
    "    test_dataloader = DataLoader(MyDataset(indexed_test_data), \n",
    "                              batch_size = batch_size, \n",
    "                              shuffle = False, \n",
    "                              num_workers = 6,\n",
    "                              drop_last = False,\n",
    "                              collate_fn = lambda data_batch: data_maker.generate_batch(data_batch, data_type = \"test\"),\n",
    "                             )\n",
    "    \n",
    "    pred_sample_list = []\n",
    "    for batch_test_data in tqdm(test_dataloader, desc = \"Predicting\"):\n",
    "        if config[\"encoder\"] == \"BERT\":\n",
    "            sample_list, batch_input_ids, \\\n",
    "            batch_attention_mask, batch_token_type_ids, \\\n",
    "            tok2char_span_list, _, _, _ = batch_test_data\n",
    "\n",
    "            batch_input_ids, \\\n",
    "            batch_attention_mask, \\\n",
    "            batch_token_type_ids = (batch_input_ids.to(device), \n",
    "                                      batch_attention_mask.to(device), \n",
    "                                      batch_token_type_ids.to(device))\n",
    "\n",
    "        elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "            sample_list, batch_input_ids, tok2char_span_list, _, _, _ = batch_test_data\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if config[\"encoder\"] == \"BERT\":\n",
    "                batch_ent_shaking_outputs, \\\n",
    "                batch_head_rel_shaking_outputs, \\\n",
    "                batch_tail_rel_shaking_outputs = rel_extractor(batch_input_ids, \n",
    "                                                          batch_attention_mask, \n",
    "                                                          batch_token_type_ids, \n",
    "                                                         )\n",
    "            elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "                batch_ent_shaking_outputs, \\\n",
    "                batch_head_rel_shaking_outputs, \\\n",
    "                batch_tail_rel_shaking_outputs = rel_extractor(batch_input_ids)\n",
    "\n",
    "        batch_ent_shaking_tag, \\\n",
    "        batch_head_rel_shaking_tag, \\\n",
    "        batch_tail_rel_shaking_tag = torch.argmax(batch_ent_shaking_outputs, dim = -1), \\\n",
    "                                     torch.argmax(batch_head_rel_shaking_outputs, dim = -1), \\\n",
    "                                     torch.argmax(batch_tail_rel_shaking_outputs, dim = -1)\n",
    "\n",
    "        for ind in range(len(sample_list)):\n",
    "            gold_sample = sample_list[ind]\n",
    "            text = gold_sample[\"text\"]\n",
    "            text_id = gold_sample[\"id\"]\n",
    "            tok2char_span = tok2char_span_list[ind]\n",
    "            ent_shaking_tag, \\\n",
    "            head_rel_shaking_tag, \\\n",
    "            tail_rel_shaking_tag = batch_ent_shaking_tag[ind], \\\n",
    "                                    batch_head_rel_shaking_tag[ind], \\\n",
    "                                    batch_tail_rel_shaking_tag[ind]\n",
    "            \n",
    "            tok_offset, char_offset = 0, 0\n",
    "            if split_test_data:\n",
    "                tok_offset, char_offset = gold_sample[\"tok_offset\"], gold_sample[\"char_offset\"]\n",
    "            rel_list = handshaking_tagger.decode_rel_fr_shaking_tag(text, \n",
    "                                                                    ent_shaking_tag, \n",
    "                                                                    head_rel_shaking_tag, \n",
    "                                                                    tail_rel_shaking_tag, \n",
    "                                                                    tok2char_span, \n",
    "                                                                    tok_offset = tok_offset, char_offset = char_offset)\n",
    "            pred_sample_list.append({\n",
    "                \"text\": text,\n",
    "                \"id\": text_id,\n",
    "                \"relation_list\": rel_list,\n",
    "            })\n",
    "            \n",
    "    # merge\n",
    "    text_id2rel_list = {}\n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        if text_id not in text_id2rel_list:\n",
    "            text_id2rel_list[text_id] = sample[\"relation_list\"]\n",
    "        else:\n",
    "            text_id2rel_list[text_id].extend(sample[\"relation_list\"])\n",
    "\n",
    "    text_id2text = {sample[\"id\"]:sample[\"text\"] for sample in ori_test_data}\n",
    "    merged_pred_sample_list = []\n",
    "    for text_id, rel_list in text_id2rel_list.items():\n",
    "        merged_pred_sample_list.append({\n",
    "            \"id\": text_id,\n",
    "            \"text\": text_id2text[text_id],\n",
    "            \"relation_list\": filter_duplicates(rel_list),\n",
    "        })\n",
    "        \n",
    "    return merged_pred_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_prf(pred_sample_list, gold_test_data, pattern = \"only_head\"):\n",
    "    text_id2gold_n_pred = {}\n",
    "    for sample in gold_test_data:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id] = {\n",
    "            \"gold_relation_list\": sample[\"relation_list\"],\n",
    "        }\n",
    "    \n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id][\"pred_relation_list\"] = sample[\"relation_list\"]\n",
    "\n",
    "    correct_num, pred_num, gold_num = 0, 0, 0\n",
    "    for gold_n_pred in text_id2gold_n_pred.values():\n",
    "        gold_rel_list = gold_n_pred[\"gold_relation_list\"]\n",
    "        pred_rel_list = gold_n_pred[\"pred_relation_list\"] if \"pred_relation_list\" in gold_n_pred else []\n",
    "        if pattern == \"only_head\":\n",
    "            gold_rel_set = set([\"{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], rel[\"predicate\"], rel[\"obj_tok_span\"][0]) for rel in gold_rel_list])\n",
    "            pred_rel_set = set([\"{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], rel[\"predicate\"], rel[\"obj_tok_span\"][0]) for rel in pred_rel_list])\n",
    "        elif pattern == \"head_n_tail\":\n",
    "            gold_rel_set = set([\"{}\\u2E80{}\\u2E80{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], rel[\"subj_tok_span\"][1], rel[\"predicate\"], rel[\"obj_tok_span\"][0], rel[\"obj_tok_span\"][1]) for rel in gold_rel_list])\n",
    "            pred_rel_set = set([\"{}\\u2E80{}\\u2E80{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0], rel[\"subj_tok_span\"][1], rel[\"predicate\"], rel[\"obj_tok_span\"][0], rel[\"obj_tok_span\"][1]) for rel in pred_rel_list])\n",
    "        elif pattern == \"whole_text\":\n",
    "            gold_rel_set = set([\"{}\\u2E80{}\\u2E80{}\".format(rel[\"subject\"], rel[\"predicate\"], rel[\"object\"]) for rel in gold_rel_list])\n",
    "            pred_rel_set = set([\"{}\\u2E80{}\\u2E80{}\".format(rel[\"subject\"], rel[\"predicate\"], rel[\"object\"]) for rel in pred_rel_list])\n",
    "            \n",
    "        for rel_str in pred_rel_set:\n",
    "            if rel_str in gold_rel_set:\n",
    "                correct_num += 1\n",
    "\n",
    "        pred_num += len(pred_rel_set)\n",
    "        gold_num += len(gold_rel_set)\n",
    "#     print((correct_num, pred_num, gold_num))\n",
    "    prf = metrics.get_prf_scores(correct_num, pred_num, gold_num)\n",
    "    return prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed train or valid data: 878it [00:00, 8712.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 3vjqnz28, model state model_state_dict_15.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed train or valid data: 3266it [00:00, 9486.94it/s]\n",
      "Predicting: 100%|██████████| 205/205 [01:45<00:00,  1.95it/s]\n",
      "Generate indexed train or valid data: 1045it [00:00, 8719.43it/s]\n",
      "Predicting: 100%|██████████| 66/66 [00:41<00:00,  1.60it/s]\n",
      "Generate indexed train or valid data: 978it [00:00, 8757.29it/s]\n",
      "Predicting: 100%|██████████| 62/62 [00:44<00:00,  1.38it/s]\n",
      "Generate indexed train or valid data: 5000it [00:00, 5445.38it/s]\n",
      "Predicting: 100%|██████████| 313/313 [03:32<00:00,  1.48it/s]\n",
      "Generate indexed train or valid data: 312it [00:00, 7283.84it/s]\n",
      "Predicting: 100%|██████████| 20/20 [00:13<00:00,  1.43it/s]\n",
      "Generate indexed train or valid data: 291it [00:00, 6813.12it/s]\n",
      "Predicting: 100%|██████████| 19/19 [00:14<00:00,  1.31it/s]\n",
      "Generate indexed train or valid data: 1297it [00:00, 7689.27it/s]\n",
      "Predicting: 100%|██████████| 82/82 [00:57<00:00,  1.43it/s]\n",
      "Generate indexed train or valid data: 108it [00:00, 4603.64it/s]\n",
      "Predicting: 100%|██████████| 7/7 [00:06<00:00,  1.14it/s]\n",
      "Generate indexed train or valid data: 3244it [00:00, 7180.47it/s]\n",
      "Predicting: 100%|██████████| 203/203 [02:19<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'../results/nyt_single/3vjqnz28/test_triples_1_res_15.json': 2999,\n",
      " '../results/nyt_single/3vjqnz28/test_triples_2_res_15.json': 995,\n",
      " '../results/nyt_single/3vjqnz28/test_triples_3_res_15.json': 298,\n",
      " '../results/nyt_single/3vjqnz28/test_triples_4_res_15.json': 281,\n",
      " '../results/nyt_single/3vjqnz28/test_triples_5_res_15.json': 104,\n",
      " '../results/nyt_single/3vjqnz28/test_triples_epo_res_15.json': 932,\n",
      " '../results/nyt_single/3vjqnz28/test_triples_normal_res_15.json': 3023,\n",
      " '../results/nyt_single/3vjqnz28/test_triples_res_15.json': 4677,\n",
      " '../results/nyt_single/3vjqnz28/test_triples_seo_res_15.json': 1240}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "res_dict = {}\n",
    "predict_statistics = {}\n",
    "\n",
    "for run_id, model_path_list in run_id2model_state_paths.items():\n",
    "    save_dir4run = os.path.join(save_res_dir, run_id)\n",
    "    if config[\"save_res\"] and not os.path.exists(save_dir4run):\n",
    "        os.makedirs(save_dir4run)\n",
    "\n",
    "    for model_state_path in model_path_list:\n",
    "        # load model state\n",
    "        rel_extractor.load_state_dict(torch.load(model_state_path))\n",
    "        rel_extractor.eval()\n",
    "        print(\"run_id: {}, model state {} loaded\".format(run_id, model_state_path.split(\"/\")[-1]))\n",
    "        \n",
    "        for file_name, short_data in test_data_dict.items():\n",
    "            res_num = re.search(\"(\\d+)\", model_state_path.split(\"/\")[-1]).group(1)\n",
    "            save_path = os.path.join(save_dir4run, \"{}_res_{}.json\".format(file_name, res_num))\n",
    "\n",
    "            if os.path.exists(save_path):\n",
    "                pred_sample_list = [json.loads(line) for line in open(save_path, \"r\", encoding = \"utf-8\")]\n",
    "                print(\"{} already exists, load it directly!\".format(save_path))\n",
    "            else:\n",
    "                # predict\n",
    "                ori_test_data = ori_test_data_dict[file_name]\n",
    "                pred_sample_list = predict(short_data, ori_test_data)\n",
    "\n",
    "            res_dict[save_path] = pred_sample_list\n",
    "            predict_statistics[save_path] = len([s for s in pred_sample_list if len(s[\"relation_list\"]) > 0])\n",
    "pprint(predict_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check char span: 100%|██████████| 3266/3266 [00:00<00:00, 176603.41it/s]\n",
      "check char span: 100%|██████████| 1045/1045 [00:00<00:00, 116344.54it/s]\n",
      "check char span: 100%|██████████| 978/978 [00:00<00:00, 93134.80it/s]\n",
      "check char span: 100%|██████████| 5000/5000 [00:00<00:00, 73273.45it/s]\n",
      "check char span: 100%|██████████| 312/312 [00:00<00:00, 21854.09it/s]\n",
      "check char span: 100%|██████████| 291/291 [00:00<00:00, 60853.69it/s]\n",
      "check char span: 100%|██████████| 1297/1297 [00:00<00:00, 88886.18it/s]\n",
      "check char span: 100%|██████████| 108/108 [00:00<00:00, 38281.49it/s]\n",
      "check char span: 100%|██████████| 3244/3244 [00:00<00:00, 174406.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "for path, res in res_dict.items():\n",
    "    for sample in tqdm(res, desc = \"check char span\"):\n",
    "        text = sample[\"text\"]\n",
    "        for rel in sample[\"relation_list\"]:\n",
    "            assert rel[\"subject\"] == text[rel[\"subj_char_span\"][0]:rel[\"subj_char_span\"][1]]\n",
    "            assert rel[\"object\"] == text[rel[\"obj_char_span\"][0]:rel[\"obj_char_span\"][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "if config[\"save_res\"]:\n",
    "    for path, res in res_dict.items():\n",
    "        with open(path, \"w\", encoding = \"utf-8\") as file_out:\n",
    "            for sample in tqdm(res, desc = \"Output\"):\n",
    "                if len(sample[\"relation_list\"]) == 0:\n",
    "                    continue\n",
    "                json_line = json.dumps(sample, ensure_ascii = False)     \n",
    "                file_out.write(\"{}\\n\".format(json_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Results -----------------------\n",
      "{'../results/nyt_single/3vjqnz28/test_triples_1_res_15.json': (0.7699239650802374,\n",
      "                                                               0.842786683107249,\n",
      "                                                               0.8047093450567745),\n",
      " '../results/nyt_single/3vjqnz28/test_triples_2_res_15.json': (0.8682247637990618,\n",
      "                                                               0.8354066985645533,\n",
      "                                                               0.8514996341855293),\n",
      " '../results/nyt_single/3vjqnz28/test_triples_3_res_15.json': (0.8884976525820553,\n",
      "                                                               0.8087606837605973,\n",
      "                                                               0.8467561520752952),\n",
      " '../results/nyt_single/3vjqnz28/test_triples_4_res_15.json': (0.8881057268721684,\n",
      "                                                               0.8659793814432245,\n",
      "                                                               0.8769030012548467),\n",
      " '../results/nyt_single/3vjqnz28/test_triples_5_res_15.json': (0.8618307426596092,\n",
      "                                                               0.7381656804732636,\n",
      "                                                               0.795219123456148),\n",
      " '../results/nyt_single/3vjqnz28/test_triples_epo_res_15.json': (0.8750423298340374,\n",
      "                                                                 0.8386887374228874,\n",
      "                                                                 0.85647994691718),\n",
      " '../results/nyt_single/3vjqnz28/test_triples_normal_res_15.json': (0.7698895027624096,\n",
      "                                                                    0.8412315122245445,\n",
      "                                                                    0.8039809605731817),\n",
      " '../results/nyt_single/3vjqnz28/test_triples_res_15.json': (0.8297244094488087,\n",
      "                                                             0.8315659679408035,\n",
      "                                                             0.8306441679509751),\n",
      " '../results/nyt_single/3vjqnz28/test_triples_seo_res_15.json': (0.8823692992213326,\n",
      "                                                                 0.8201085551821964,\n",
      "                                                                 0.8501004688046991)}\n"
     ]
    }
   ],
   "source": [
    "# score\n",
    "if config[\"score\"]:\n",
    "    res_dict = {}\n",
    "    for file_path, pred_samples in res_dict.items():\n",
    "        run_id = file_path.split(\"/\")[-2]\n",
    "        file_name = re.search(\"(.*?)_res_\\d+\\.json\", file_path.split(\"/\")[-1]).group(1)\n",
    "        gold_test_data = ori_test_data_dict[file_name]\n",
    "        prf = get_test_prf(pred_samples, gold_test_data, pattern = config[\"correct\"])\n",
    "        if run_id not in \n",
    "        filepath2scores[file_path] = prf\n",
    "    print(\"---------------- Results -----------------------\")\n",
    "    pprint(filepath2scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
