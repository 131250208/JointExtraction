{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "from common.utils import Preprocessor\n",
    "import yaml\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "config = yaml.load(open(\"build_data_config.yaml\", \"r\"), Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = config[\"exp_name\"]\n",
    "data_in_dir = os.path.join(config[\"data_in_dir\"], exp_name)\n",
    "data_out_dir = os.path.join(config[\"data_out_dir\"], exp_name)\n",
    "if not os.path.exists(data_out_dir):\n",
    "    os.makedirs(data_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name2data = {}\n",
    "for path, folds, files in os.walk(data_in_dir):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        file_name = re.match(\"(.*?)\\.json\", file_name).group(1)\n",
    "        file_name2data[file_name] = json.load(open(file_path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @specific\n",
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "elif config[\"encoder\"] == \"BiLSTM\":\n",
    "    tokenize = lambda text: text.split(\" \")\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = tokenize(text)\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func = tokenize, \n",
    "                            get_tok2char_span_map_func = get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_format = config[\"ori_data_format\"]\n",
    "if ori_format != \"tplinker\": # if tplinker, skip transforming\n",
    "    for file_name, data in file_name2data.items():\n",
    "        if \"train\" in file_name:\n",
    "            data_type = \"train\"\n",
    "        if \"valid\" in file_name:\n",
    "            data_type = \"valid\"\n",
    "        if \"test\" in file_name:\n",
    "            data_type = \"test\"\n",
    "        data = preprocessor.transform_data(data, ori_format = ori_format, dataset_type = data_type, add_id = True)\n",
    "        file_name2data[file_name] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Clean and Add Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check token level span\n",
    "def check_tok_span(data):\n",
    "    def extr_ent(text, tok_span, tok2char_span):\n",
    "        char_span_list = tok2char_span[tok_span[0]:tok_span[1]]\n",
    "        char_span = (char_span_list[0][0], char_span_list[-1][1])\n",
    "        decoded_ent = text[char_span[0]:char_span[1]]\n",
    "        return decoded_ent\n",
    "\n",
    "    span_error_memory = set()\n",
    "    for sample in tqdm(data, desc = \"check tok spans\"):\n",
    "        text = sample[\"text\"]\n",
    "        tok2char_span = get_tok2char_span_map(text)\n",
    "        for rel in sample[\"relation_list\"]:\n",
    "            subj_tok_span, obj_tok_span = rel[\"subj_tok_span\"], rel[\"obj_tok_span\"]\n",
    "            if extr_ent(text, subj_tok_span, tok2char_span) != rel[\"subject\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, subj_tok_span, tok2char_span), rel[\"subject\"]))\n",
    "            if extr_ent(text, obj_tok_span, tok2char_span) != rel[\"object\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, obj_tok_span, tok2char_span), rel[\"object\"]))\n",
    "                \n",
    "    return span_error_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clean data: 100%|██████████| 5035/5035 [00:01<00:00, 4245.25it/s]\n",
      "clean data w char spans: 100%|██████████| 5035/5035 [00:00<00:00, 14035.44it/s]\n",
      "Adding token level spans: 100%|██████████| 5035/5035 [00:05<00:00, 889.99it/s] \n",
      "collect relations: 100%|██████████| 5035/5035 [00:00<00:00, 116020.70it/s]\n",
      "check tok spans: 100%|██████████| 5035/5035 [00:04<00:00, 1068.27it/s]\n",
      "clean data: 100%|██████████| 5040/5040 [00:01<00:00, 4040.79it/s]\n",
      "clean data w char spans: 100%|██████████| 5040/5040 [00:00<00:00, 28414.10it/s]\n",
      "Adding token level spans: 100%|██████████| 5040/5040 [00:05<00:00, 959.86it/s] \n",
      "collect relations: 100%|██████████| 5040/5040 [00:00<00:00, 143991.80it/s]\n",
      "check tok spans: 100%|██████████| 5040/5040 [00:03<00:00, 1365.16it/s]\n",
      "clean data: 100%|██████████| 720/720 [00:00<00:00, 4555.07it/s]\n",
      "clean data w char spans: 100%|██████████| 720/720 [00:00<00:00, 22577.00it/s]\n",
      "Adding token level spans: 100%|██████████| 720/720 [00:00<00:00, 949.33it/s]\n",
      "collect relations: 100%|██████████| 720/720 [00:00<00:00, 223779.09it/s]\n",
      "check tok spans: 100%|██████████| 720/720 [00:00<00:00, 1173.79it/s]\n",
      "clean data: 100%|██████████| 5035/5035 [00:01<00:00, 3233.66it/s]\n",
      "clean data w char spans: 100%|██████████| 5035/5035 [00:00<00:00, 17416.27it/s]\n",
      "Adding token level spans: 100%|██████████| 5035/5035 [00:06<00:00, 788.32it/s] \n",
      "collect relations: 100%|██████████| 5035/5035 [00:00<00:00, 136914.13it/s]\n",
      "check tok spans: 100%|██████████| 5035/5035 [00:03<00:00, 1297.19it/s]\n",
      "clean data: 100%|██████████| 720/720 [00:00<00:00, 4825.97it/s]\n",
      "clean data w char spans: 100%|██████████| 720/720 [00:00<00:00, 32777.60it/s]\n",
      "Adding token level spans: 100%|██████████| 720/720 [00:00<00:00, 838.34it/s] \n",
      "collect relations: 100%|██████████| 720/720 [00:00<00:00, 303721.10it/s]\n",
      "check tok spans: 100%|██████████| 720/720 [00:00<00:00, 1471.51it/s]\n",
      "clean data: 100%|██████████| 720/720 [00:00<00:00, 3801.25it/s]\n",
      "clean data w char spans: 100%|██████████| 720/720 [00:00<00:00, 19289.08it/s]\n",
      "Adding token level spans: 100%|██████████| 720/720 [00:00<00:00, 761.93it/s]\n",
      "collect relations: 100%|██████████| 720/720 [00:00<00:00, 265625.73it/s]\n",
      "check tok spans: 100%|██████████| 720/720 [00:00<00:00, 1329.18it/s]\n",
      "clean data: 100%|██████████| 715/715 [00:00<00:00, 4883.18it/s]\n",
      "clean data w char spans: 100%|██████████| 715/715 [00:00<00:00, 27589.03it/s]\n",
      "Adding token level spans: 100%|██████████| 715/715 [00:00<00:00, 889.39it/s]\n",
      "collect relations: 100%|██████████| 715/715 [00:00<00:00, 47595.98it/s]\n",
      "check tok spans: 100%|██████████| 715/715 [00:00<00:00, 1208.79it/s]\n",
      "clean data: 100%|██████████| 5035/5035 [00:01<00:00, 3719.41it/s]\n",
      "clean data w char spans: 100%|██████████| 5035/5035 [00:00<00:00, 17380.00it/s]\n",
      "Adding token level spans: 100%|██████████| 5035/5035 [00:05<00:00, 927.59it/s] \n",
      "collect relations: 100%|██████████| 5035/5035 [00:00<00:00, 151402.09it/s]\n",
      "check tok spans: 100%|██████████| 5035/5035 [00:04<00:00, 1045.16it/s]\n",
      "clean data: 100%|██████████| 5035/5035 [00:01<00:00, 3931.62it/s]\n",
      "clean data w char spans: 100%|██████████| 5035/5035 [00:00<00:00, 20811.99it/s]\n",
      "Adding token level spans: 100%|██████████| 5035/5035 [00:05<00:00, 930.74it/s] \n",
      "collect relations: 100%|██████████| 5035/5035 [00:00<00:00, 149299.89it/s]\n",
      "check tok spans: 100%|██████████| 5035/5035 [00:04<00:00, 1217.73it/s]\n",
      "clean data: 100%|██████████| 720/720 [00:00<00:00, 4981.58it/s]\n",
      "clean data w char spans: 100%|██████████| 720/720 [00:00<00:00, 17523.73it/s]\n",
      "Adding token level spans: 100%|██████████| 720/720 [00:00<00:00, 907.66it/s] \n",
      "collect relations: 100%|██████████| 720/720 [00:00<00:00, 287801.28it/s]\n",
      "check tok spans: 100%|██████████| 720/720 [00:00<00:00, 1346.79it/s]\n",
      "clean data: 100%|██████████| 720/720 [00:00<00:00, 4856.88it/s]\n",
      "clean data w char spans: 100%|██████████| 720/720 [00:00<00:00, 22260.46it/s]\n",
      "Adding token level spans: 100%|██████████| 720/720 [00:00<00:00, 1056.31it/s]\n",
      "collect relations: 100%|██████████| 720/720 [00:00<00:00, 267840.26it/s]\n",
      "check tok spans: 100%|██████████| 720/720 [00:00<00:00, 1460.96it/s]\n",
      "clean data: 100%|██████████| 5035/5035 [00:01<00:00, 4280.89it/s]\n",
      "clean data w char spans: 100%|██████████| 5035/5035 [00:00<00:00, 20158.53it/s]\n",
      "Adding token level spans: 100%|██████████| 5035/5035 [00:05<00:00, 939.20it/s] \n",
      "collect relations: 100%|██████████| 5035/5035 [00:00<00:00, 175983.07it/s]\n",
      "check tok spans: 100%|██████████| 5035/5035 [00:04<00:00, 1201.49it/s]\n",
      "clean data: 100%|██████████| 720/720 [00:00<00:00, 4916.86it/s]\n",
      "clean data w char spans: 100%|██████████| 720/720 [00:00<00:00, 19211.65it/s]\n",
      "Adding token level spans: 100%|██████████| 720/720 [00:00<00:00, 900.26it/s] \n",
      "collect relations: 100%|██████████| 720/720 [00:00<00:00, 249991.63it/s]\n",
      "check tok spans: 100%|██████████| 720/720 [00:00<00:00, 1101.76it/s]\n",
      "clean data: 100%|██████████| 5035/5035 [00:01<00:00, 3786.85it/s]\n",
      "clean data w char spans: 100%|██████████| 5035/5035 [00:00<00:00, 14259.17it/s]\n",
      "Adding token level spans: 100%|██████████| 5035/5035 [00:06<00:00, 834.72it/s] \n",
      "collect relations: 100%|██████████| 5035/5035 [00:00<00:00, 91040.59it/s]\n",
      "check tok spans: 100%|██████████| 5035/5035 [00:04<00:00, 1146.06it/s]\n",
      "clean data: 100%|██████████| 720/720 [00:00<00:00, 3710.42it/s]\n",
      "clean data w char spans: 100%|██████████| 720/720 [00:00<00:00, 21324.56it/s]\n",
      "Adding token level spans: 100%|██████████| 720/720 [00:00<00:00, 740.13it/s]\n",
      "collect relations: 100%|██████████| 720/720 [00:00<00:00, 251805.13it/s]\n",
      "check tok spans: 100%|██████████| 720/720 [00:00<00:00, 1278.22it/s]\n",
      "clean data: 100%|██████████| 5035/5035 [00:01<00:00, 3549.70it/s]\n",
      "clean data w char spans: 100%|██████████| 5035/5035 [00:00<00:00, 13876.56it/s]\n",
      "Adding token level spans: 100%|██████████| 5035/5035 [00:05<00:00, 904.64it/s] \n",
      "collect relations: 100%|██████████| 5035/5035 [00:00<00:00, 142461.30it/s]\n",
      "check tok spans: 100%|██████████| 5035/5035 [00:04<00:00, 1139.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_data_0': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'train_data_1': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'train_data_2': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'train_data_3': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'train_data_4': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'train_data_5': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'train_data_6': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'train_data_7': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'valid_data_0': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'valid_data_1': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'valid_data_2': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'valid_data_3': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'valid_data_4': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'valid_data_5': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'valid_data_6': {'char_span_error': 0, 'tok_span_error': 0},\n",
      " 'valid_data_7': {'char_span_error': 0, 'tok_span_error': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clean, add char span, tok span\n",
    "# collect relations\n",
    "# check tok spans\n",
    "rel_set = set()\n",
    "error_statistics = {}\n",
    "for file_name, data in file_name2data.items():\n",
    "    assert len(data) > 0\n",
    "    if \"relation_list\" in data[0]: # train or valid data\n",
    "        # rm redundant whitespaces\n",
    "        # separate by whitespaces\n",
    "        data = preprocessor.clean_data_wo_span(data, separate = config[\"separate_char_by_white\"])\n",
    "        error_statistics[file_name] = {}\n",
    "        # add char span\n",
    "        if config[\"add_char_span\"]:\n",
    "            data, miss_sample_list = preprocessor.add_char_span(data, config[\"ignore_subword\"])\n",
    "            error_statistics[file_name][\"miss_samples\"] = len(miss_sample_list)\n",
    "            \n",
    "        # clean\n",
    "        data, bad_samples_w_char_span_error = preprocessor.clean_data_w_span(data)\n",
    "        error_statistics[file_name][\"char_span_error\"] = len(bad_samples_w_char_span_error)\n",
    "        \n",
    "        # add tok span\n",
    "        data = preprocessor.add_tok_span(data)\n",
    "        \n",
    "        # collect relations\n",
    "        for sample in tqdm(data, desc = \"collect relations\"):\n",
    "            for rel in sample[\"relation_list\"]:\n",
    "                rel_set.add(rel[\"predicate\"])\n",
    "        \n",
    "        # check tok span\n",
    "        if config[\"check_tok_span\"]:\n",
    "            span_error_memory = check_tok_span(data)\n",
    "            if len(span_error_memory) > 0:\n",
    "                print(span_error_memory)\n",
    "            error_statistics[file_name][\"tok_span_error\"] = len(span_error_memory)\n",
    "            \n",
    "        file_name2data[file_name] = data\n",
    "pprint(error_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train_data_3 is output to ../data4bert/xf_event_extr_t1_/train_data_3.json\n",
      "INFO:root:train_data_7 is output to ../data4bert/xf_event_extr_t1_/train_data_7.json\n",
      "INFO:root:valid_data_4 is output to ../data4bert/xf_event_extr_t1_/valid_data_4.json\n",
      "INFO:root:train_data_0 is output to ../data4bert/xf_event_extr_t1_/train_data_0.json\n",
      "INFO:root:test_data is output to ../data4bert/xf_event_extr_t1_/test_data.json\n",
      "INFO:root:valid_data_5 is output to ../data4bert/xf_event_extr_t1_/valid_data_5.json\n",
      "INFO:root:valid_data_6 is output to ../data4bert/xf_event_extr_t1_/valid_data_6.json\n",
      "INFO:root:valid_data_7 is output to ../data4bert/xf_event_extr_t1_/valid_data_7.json\n",
      "INFO:root:train_data_2 is output to ../data4bert/xf_event_extr_t1_/train_data_2.json\n",
      "INFO:root:train_data_6 is output to ../data4bert/xf_event_extr_t1_/train_data_6.json\n",
      "INFO:root:valid_data_2 is output to ../data4bert/xf_event_extr_t1_/valid_data_2.json\n",
      "INFO:root:valid_data_1 is output to ../data4bert/xf_event_extr_t1_/valid_data_1.json\n",
      "INFO:root:train_data_4 is output to ../data4bert/xf_event_extr_t1_/train_data_4.json\n",
      "INFO:root:valid_data_0 is output to ../data4bert/xf_event_extr_t1_/valid_data_0.json\n",
      "INFO:root:train_data_5 is output to ../data4bert/xf_event_extr_t1_/train_data_5.json\n",
      "INFO:root:valid_data_3 is output to ../data4bert/xf_event_extr_t1_/valid_data_3.json\n",
      "INFO:root:train_data_1 is output to ../data4bert/xf_event_extr_t1_/train_data_1.json\n",
      "INFO:root:rel2id is output to ../data4bert/xf_event_extr_t1_/rel2id.json\n",
      "INFO:root:data_statistics is output to ../data4bert/xf_event_extr_t1_/data_statistics.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relation_num': 8,\n",
      " 'test_data': 1200,\n",
      " 'train_data_0': 5035,\n",
      " 'train_data_1': 5035,\n",
      " 'train_data_2': 5035,\n",
      " 'train_data_3': 5035,\n",
      " 'train_data_4': 5035,\n",
      " 'train_data_5': 5035,\n",
      " 'train_data_6': 5035,\n",
      " 'train_data_7': 5040,\n",
      " 'valid_data_0': 720,\n",
      " 'valid_data_1': 720,\n",
      " 'valid_data_2': 720,\n",
      " 'valid_data_3': 720,\n",
      " 'valid_data_4': 720,\n",
      " 'valid_data_5': 720,\n",
      " 'valid_data_6': 720,\n",
      " 'valid_data_7': 715}\n"
     ]
    }
   ],
   "source": [
    "rel_set = sorted(rel_set)\n",
    "rel2id = {rel:ind for ind, rel in enumerate(rel_set)}\n",
    "data_statistics = {\n",
    "    \"relation_num\": len(rel2id),\n",
    "}\n",
    "\n",
    "for file_name, data in file_name2data.items():\n",
    "    data_path = os.path.join(data_out_dir, \"{}.json\".format(file_name))\n",
    "    json.dump(data, open(data_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "    logging.info(\"{} is output to {}\".format(file_name, data_path))\n",
    "    data_statistics[file_name] = len(data)\n",
    "\n",
    "rel2id_path = os.path.join(data_out_dir, \"rel2id.json\")\n",
    "json.dump(rel2id, open(rel2id_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "logging.info(\"rel2id is output to {}\".format(rel2id_path))\n",
    "\n",
    "data_statistics_path = os.path.join(data_out_dir, \"data_statistics.txt\")\n",
    "json.dump(data_statistics, open(data_statistics_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "logging.info(\"data_statistics is output to {}\".format(data_statistics_path)) \n",
    "\n",
    "pprint(data_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrate WordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    all_data = []\n",
    "    for data in list(file_name2data.values()):\n",
    "        all_data.extend(data)\n",
    "        \n",
    "    token2num = {}\n",
    "    for sample in tqdm(all_data, desc = \"Tokenizing\"):\n",
    "        text = sample['text']\n",
    "        for tok in tokenize(text):\n",
    "            token2num[tok] = token2num.get(tok, 0) + 1\n",
    "    \n",
    "    token2num = dict(sorted(token2num.items(), key = lambda x: x[1], reverse = True))\n",
    "    max_token_num = 50000\n",
    "    token_set = set()\n",
    "    for tok, num in tqdm(token2num.items(), desc = \"Filter uncommon words\"):\n",
    "        if num < 3: # filter words with a frequency of less than 3\n",
    "            continue\n",
    "        token_set.add(tok)\n",
    "        if len(token_set) == max_token_num:\n",
    "            break\n",
    "        \n",
    "    token2idx = {tok:idx + 2 for idx, tok in enumerate(sorted(token_set))}\n",
    "    token2idx[\"<PAD>\"] = 0\n",
    "    token2idx[\"<UNK>\"] = 1\n",
    "#     idx2token = {idx:tok for tok, idx in token2idx.items()}\n",
    "    \n",
    "    dict_path = os.path.join(data_out_dir, \"token2idx.json\")\n",
    "    json.dump(token2idx, open(dict_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "    logging.info(\"token2idx is output to {}, total token num: {}\".format(dict_path, len(token2idx))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
