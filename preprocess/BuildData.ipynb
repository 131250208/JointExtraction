{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "from common.utils import Preprocessor\n",
    "import yaml\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "config = yaml.load(open(\"build_data_config.yaml\", \"r\"), Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = config[\"exp_name\"]\n",
    "data_in_dir = os.path.join(config[\"data_in_dir\"], exp_name)\n",
    "data_out_dir = os.path.join(config[\"data_out_dir\"], exp_name)\n",
    "if not os.path.exists(data_out_dir):\n",
    "    os.makedirs(data_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name2data = {}\n",
    "for path, folds, files in os.walk(data_in_dir):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        file_name = re.match(\"(.*?)\\.json\", file_name).group(1)\n",
    "        file_name2data[file_name] = json.load(open(file_path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @specific\n",
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "elif config[\"encoder\"] == \"BiLSTM\":\n",
    "    tokenize = lambda text: text.split(\" \")\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = text.split(\" \")\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func = tokenize, \n",
    "                            get_tok2char_span_map_func = get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming data format: 4999it [00:00, 142234.29it/s]\n",
      "Clean: 100%|██████████| 4999/4999 [00:00<00:00, 80034.68it/s]\n",
      "Transforming data format: 56195it [00:00, 75528.50it/s]\n",
      "Clean: 100%|██████████| 56195/56195 [00:00<00:00, 64354.09it/s]\n",
      "Transforming data format: 5000it [00:00, 105333.71it/s]\n",
      "Clean: 100%|██████████| 5000/5000 [00:00<00:00, 61520.99it/s]\n",
      "Transforming data format: 3266it [00:00, 124173.72it/s]\n",
      "Clean: 100%|██████████| 3266/3266 [00:00<00:00, 79364.77it/s]\n",
      "Transforming data format: 978it [00:00, 71462.68it/s]\n",
      "Clean: 100%|██████████| 978/978 [00:00<00:00, 35579.17it/s]\n",
      "Transforming data format: 1297it [00:00, 93376.34it/s]\n",
      "Clean: 100%|██████████| 1297/1297 [00:00<00:00, 38406.50it/s]\n",
      "Transforming data format: 1045it [00:00, 84224.59it/s]\n",
      "Clean: 100%|██████████| 1045/1045 [00:00<00:00, 52454.53it/s]\n",
      "Transforming data format: 312it [00:00, 81824.73it/s]\n",
      "Clean: 100%|██████████| 312/312 [00:00<00:00, 37190.52it/s]\n",
      "Transforming data format: 291it [00:00, 71813.51it/s]\n",
      "Clean: 100%|██████████| 291/291 [00:00<00:00, 29830.44it/s]\n",
      "Transforming data format: 108it [00:00, 45576.50it/s]\n",
      "Clean: 100%|██████████| 108/108 [00:00<00:00, 17162.42it/s]\n",
      "Transforming data format: 3244it [00:00, 124128.29it/s]\n",
      "Clean: 100%|██████████| 3244/3244 [00:00<00:00, 78641.99it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_format = config[\"ori_data_format\"]\n",
    "if ori_format != \"tplinker\": # if tplinker, skip transforming\n",
    "    for file_name, data in file_name2data.items():\n",
    "        if \"train\" in file_name:\n",
    "            data_type = \"train\"\n",
    "        if \"valid\" in file_name:\n",
    "            data_type = \"valid\"\n",
    "        if \"test\" in file_name:\n",
    "            data_type = \"test\"\n",
    "        data = preprocessor.transform_data(data, ori_format = ori_format, dataset_type = data_type, add_id = True)\n",
    "        file_name2data[file_name] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Clean and Add Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check token level span\n",
    "def check_tok_span(data):\n",
    "    def extr_ent(text, tok_span, tok2char_span):\n",
    "        char_span_list = tok2char_span[tok_span[0]:tok_span[1]]\n",
    "        char_span = (char_span_list[0][0], char_span_list[-1][1])\n",
    "        decoded_ent = text[char_span[0]:char_span[1]]\n",
    "        return decoded_ent\n",
    "\n",
    "    span_error_memory = set()\n",
    "    for sample in tqdm(data, desc = \"check tok spans\"):\n",
    "        text = sample[\"text\"]\n",
    "        tok2char_span = get_tok2char_span_map(text)\n",
    "        for rel in sample[\"relation_list\"]:\n",
    "            subj_tok_span, obj_tok_span = rel[\"subj_tok_span\"], rel[\"obj_tok_span\"]\n",
    "            if extr_ent(text, subj_tok_span, tok2char_span) != rel[\"subject\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, subj_tok_span, tok2char_span), rel[\"subject\"]))\n",
    "            if extr_ent(text, obj_tok_span, tok2char_span) != rel[\"object\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, obj_tok_span, tok2char_span), rel[\"object\"]))\n",
    "                \n",
    "    return span_error_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "separate characters by white: 100%|██████████| 4999/4999 [00:00<00:00, 21878.52it/s]\n",
      "Adding char level spans: 100%|██████████| 4999/4999 [00:01<00:00, 4831.62it/s]\n",
      "cleaning: 100%|██████████| 4999/4999 [00:00<00:00, 79510.54it/s]\n",
      "Adding token level spans: 100%|██████████| 4999/4999 [00:00<00:00, 6374.41it/s]\n",
      "collect relations: 100%|██████████| 4999/4999 [00:00<00:00, 215828.69it/s]\n",
      "check tok spans: 100%|██████████| 4999/4999 [00:00<00:00, 30629.00it/s]\n",
      "separate characters by white: 100%|██████████| 56195/56195 [00:02<00:00, 21720.94it/s]\n",
      "Adding char level spans: 100%|██████████| 56195/56195 [00:09<00:00, 6157.86it/s]\n",
      "cleaning: 100%|██████████| 56195/56195 [00:00<00:00, 88279.36it/s]\n",
      "Adding token level spans: 100%|██████████| 56195/56195 [00:09<00:00, 5843.96it/s]\n",
      "collect relations: 100%|██████████| 56195/56195 [00:00<00:00, 291039.85it/s]\n",
      "check tok spans: 100%|██████████| 56195/56195 [00:01<00:00, 31798.50it/s]\n",
      "separate characters by white: 100%|██████████| 5000/5000 [00:00<00:00, 21826.92it/s]\n",
      "Adding char level spans: 100%|██████████| 5000/5000 [00:00<00:00, 7020.77it/s]\n",
      "cleaning: 100%|██████████| 5000/5000 [00:00<00:00, 85490.57it/s]\n",
      "Adding token level spans: 100%|██████████| 5000/5000 [00:00<00:00, 6731.68it/s]\n",
      "collect relations: 100%|██████████| 5000/5000 [00:00<00:00, 278661.67it/s]\n",
      "check tok spans: 100%|██████████| 5000/5000 [00:00<00:00, 31305.68it/s]\n",
      "separate characters by white: 100%|██████████| 3266/3266 [00:00<00:00, 24605.59it/s]\n",
      "Adding char level spans: 100%|██████████| 3266/3266 [00:00<00:00, 7908.51it/s]\n",
      "cleaning: 100%|██████████| 3266/3266 [00:00<00:00, 112147.53it/s]\n",
      "Adding token level spans: 100%|██████████| 3266/3266 [00:00<00:00, 6876.73it/s]\n",
      "collect relations: 100%|██████████| 3266/3266 [00:00<00:00, 338320.50it/s]\n",
      "check tok spans: 100%|██████████| 3266/3266 [00:00<00:00, 34139.72it/s]\n",
      "separate characters by white: 100%|██████████| 978/978 [00:00<00:00, 16539.84it/s]\n",
      "Adding char level spans: 100%|██████████| 978/978 [00:00<00:00, 6665.91it/s]\n",
      "cleaning: 100%|██████████| 978/978 [00:00<00:00, 52462.33it/s]\n",
      "Adding token level spans: 100%|██████████| 978/978 [00:00<00:00, 6267.00it/s]\n",
      "collect relations: 100%|██████████| 978/978 [00:00<00:00, 184701.22it/s]\n",
      "check tok spans: 100%|██████████| 978/978 [00:00<00:00, 25578.53it/s]\n",
      "separate characters by white: 100%|██████████| 1297/1297 [00:00<00:00, 17013.38it/s]\n",
      "Adding char level spans: 100%|██████████| 1297/1297 [00:00<00:00, 5873.76it/s]\n",
      "cleaning: 100%|██████████| 1297/1297 [00:00<00:00, 52901.43it/s]\n",
      "Adding token level spans: 100%|██████████| 1297/1297 [00:00<00:00, 6092.97it/s]\n",
      "collect relations: 100%|██████████| 1297/1297 [00:00<00:00, 174852.54it/s]\n",
      "check tok spans: 100%|██████████| 1297/1297 [00:00<00:00, 25922.10it/s]\n",
      "separate characters by white: 100%|██████████| 1045/1045 [00:00<00:00, 19754.76it/s]\n",
      "Adding char level spans: 100%|██████████| 1045/1045 [00:00<00:00, 6999.07it/s]\n",
      "cleaning: 100%|██████████| 1045/1045 [00:00<00:00, 71111.81it/s]\n",
      "Adding token level spans: 100%|██████████| 1045/1045 [00:00<00:00, 5936.43it/s]\n",
      "collect relations: 100%|██████████| 1045/1045 [00:00<00:00, 225883.72it/s]\n",
      "check tok spans: 100%|██████████| 1045/1045 [00:00<00:00, 29260.89it/s]\n",
      "separate characters by white: 100%|██████████| 312/312 [00:00<00:00, 16126.37it/s]\n",
      "Adding char level spans: 100%|██████████| 312/312 [00:00<00:00, 5781.59it/s]\n",
      "cleaning: 100%|██████████| 312/312 [00:00<00:00, 49748.06it/s]\n",
      "Adding token level spans: 100%|██████████| 312/312 [00:00<00:00, 6020.31it/s]\n",
      "collect relations: 100%|██████████| 312/312 [00:00<00:00, 171577.66it/s]\n",
      "check tok spans: 100%|██████████| 312/312 [00:00<00:00, 22917.29it/s]\n",
      "separate characters by white: 100%|██████████| 291/291 [00:00<00:00, 14467.57it/s]\n",
      "Adding char level spans: 100%|██████████| 291/291 [00:00<00:00, 6129.16it/s]\n",
      "cleaning: 100%|██████████| 291/291 [00:00<00:00, 41700.86it/s]\n",
      "Adding token level spans: 100%|██████████| 291/291 [00:00<00:00, 5934.21it/s]\n",
      "collect relations: 100%|██████████| 291/291 [00:00<00:00, 128154.40it/s]\n",
      "check tok spans: 100%|██████████| 291/291 [00:00<00:00, 21796.57it/s]\n",
      "separate characters by white: 100%|██████████| 108/108 [00:00<00:00, 10555.15it/s]\n",
      "Adding char level spans: 100%|██████████| 108/108 [00:00<00:00, 3308.70it/s]\n",
      "cleaning: 100%|██████████| 108/108 [00:00<00:00, 25021.26it/s]\n",
      "Adding token level spans: 100%|██████████| 108/108 [00:00<00:00, 4693.27it/s]\n",
      "collect relations: 100%|██████████| 108/108 [00:00<00:00, 79919.70it/s]\n",
      "check tok spans: 100%|██████████| 108/108 [00:00<00:00, 16551.62it/s]\n",
      "separate characters by white: 100%|██████████| 3244/3244 [00:00<00:00, 24946.60it/s]\n",
      "Adding char level spans: 100%|██████████| 3244/3244 [00:00<00:00, 8229.70it/s]\n",
      "cleaning: 100%|██████████| 3244/3244 [00:00<00:00, 117066.79it/s]\n",
      "Adding token level spans: 100%|██████████| 3244/3244 [00:01<00:00, 2335.01it/s]\n",
      "collect relations: 100%|██████████| 3244/3244 [00:00<00:00, 330949.39it/s]\n",
      "check tok spans: 100%|██████████| 3244/3244 [00:00<00:00, 34573.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_triples': {'char_span_error': 0, 'miss_samples': 0, 'tok_span_error': 0},\n",
      " 'test_triples_1': {'char_span_error': 0,\n",
      "                    'miss_samples': 0,\n",
      "                    'tok_span_error': 0},\n",
      " 'test_triples_2': {'char_span_error': 0,\n",
      "                    'miss_samples': 0,\n",
      "                    'tok_span_error': 0},\n",
      " 'test_triples_3': {'char_span_error': 0,\n",
      "                    'miss_samples': 0,\n",
      "                    'tok_span_error': 0},\n",
      " 'test_triples_4': {'char_span_error': 0,\n",
      "                    'miss_samples': 0,\n",
      "                    'tok_span_error': 0},\n",
      " 'test_triples_5': {'char_span_error': 0,\n",
      "                    'miss_samples': 0,\n",
      "                    'tok_span_error': 0},\n",
      " 'test_triples_epo': {'char_span_error': 0,\n",
      "                      'miss_samples': 0,\n",
      "                      'tok_span_error': 0},\n",
      " 'test_triples_normal': {'char_span_error': 0,\n",
      "                         'miss_samples': 0,\n",
      "                         'tok_span_error': 0},\n",
      " 'test_triples_seo': {'char_span_error': 0,\n",
      "                      'miss_samples': 0,\n",
      "                      'tok_span_error': 0},\n",
      " 'train_data': {'char_span_error': 0, 'miss_samples': 0, 'tok_span_error': 0},\n",
      " 'valid_data': {'char_span_error': 0, 'miss_samples': 0, 'tok_span_error': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clean, add char span, tok span\n",
    "# collect relations\n",
    "# check tok spans\n",
    "rel_set = set()\n",
    "error_statistics = {}\n",
    "for file_name, data in file_name2data.items():\n",
    "    assert len(data) > 0\n",
    "    if \"relation_list\" in data[0]: # train or valid data\n",
    "        # rm redundant whitespaces\n",
    "        # separate by whitespaces\n",
    "        data = preprocessor.clean_data_wo_span(data, separate = config[\"separate_char_by_white\"])\n",
    "        \n",
    "        # add char span\n",
    "        if config[\"add_char_span\"]:\n",
    "            data, miss_sample_list = preprocessor.add_char_span(data, config[\"ignore_subword\"])\n",
    "            error_statistics[file_name] = {\n",
    "                \"miss_samples\": len(miss_sample_list)\n",
    "            }\n",
    "        # clean\n",
    "        data, bad_samples_w_char_span_error = preprocessor.clean_data_w_span(data)\n",
    "        error_statistics[file_name][\"char_span_error\"] = len(bad_samples_w_char_span_error)\n",
    "        \n",
    "        # add tok span\n",
    "        data = preprocessor.add_tok_span(data)\n",
    "        \n",
    "        # collect relations\n",
    "        for sample in tqdm(data, desc = \"collect relations\"):\n",
    "            for rel in sample[\"relation_list\"]:\n",
    "                rel_set.add(rel[\"predicate\"])\n",
    "        \n",
    "        # check tok span\n",
    "        if config[\"check_tok_span\"]:\n",
    "            span_error_memory = check_tok_span(data)\n",
    "            error_statistics[file_name][\"tok_span_error\"] = len(span_error_memory)\n",
    "            \n",
    "        file_name2data[file_name] = data\n",
    "pprint(error_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:valid_data is output to ../data4bilstm/nyt_single/valid_data.json\n",
      "INFO:root:train_data is output to ../data4bilstm/nyt_single/train_data.json\n",
      "INFO:root:test_triples is output to ../data4bilstm/nyt_single/test_triples.json\n",
      "INFO:root:test_triples_normal is output to ../data4bilstm/nyt_single/test_triples_normal.json\n",
      "INFO:root:test_triples_epo is output to ../data4bilstm/nyt_single/test_triples_epo.json\n",
      "INFO:root:test_triples_seo is output to ../data4bilstm/nyt_single/test_triples_seo.json\n",
      "INFO:root:test_triples_2 is output to ../data4bilstm/nyt_single/test_triples_2.json\n",
      "INFO:root:test_triples_3 is output to ../data4bilstm/nyt_single/test_triples_3.json\n",
      "INFO:root:test_triples_4 is output to ../data4bilstm/nyt_single/test_triples_4.json\n",
      "INFO:root:test_triples_5 is output to ../data4bilstm/nyt_single/test_triples_5.json\n",
      "INFO:root:test_triples_1 is output to ../data4bilstm/nyt_single/test_triples_1.json\n",
      "INFO:root:rel2id is output to ../data4bilstm/nyt_single/rel2id.json\n",
      "INFO:root:data_statistics is output to ../data4bilstm/nyt_single/data_statistics.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relation_num': 24,\n",
      " 'test_triples': 5000,\n",
      " 'test_triples_1': 3244,\n",
      " 'test_triples_2': 1045,\n",
      " 'test_triples_3': 312,\n",
      " 'test_triples_4': 291,\n",
      " 'test_triples_5': 108,\n",
      " 'test_triples_epo': 978,\n",
      " 'test_triples_normal': 3266,\n",
      " 'test_triples_seo': 1297,\n",
      " 'train_data': 56195,\n",
      " 'valid_data': 4999}\n"
     ]
    }
   ],
   "source": [
    "rel_set = sorted(rel_set)\n",
    "rel2id = {rel:ind for ind, rel in enumerate(rel_set)}\n",
    "data_statistics = {\n",
    "    \"relation_num\": len(rel2id),\n",
    "}\n",
    "\n",
    "for file_name, data in file_name2data.items():\n",
    "    data_path = os.path.join(data_out_dir, \"{}.json\".format(file_name))\n",
    "    json.dump(data, open(data_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "    logging.info(\"{} is output to {}\".format(file_name, data_path))\n",
    "    data_statistics[file_name] = len(data)\n",
    "\n",
    "rel2id_path = os.path.join(data_out_dir, \"rel2id.json\")\n",
    "json.dump(rel2id, open(rel2id_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "logging.info(\"rel2id is output to {}\".format(rel2id_path))\n",
    "\n",
    "data_statistics_path = os.path.join(data_out_dir, \"data_statistics.txt\")\n",
    "json.dump(data_statistics, open(data_statistics_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "logging.info(\"data_statistics is output to {}\".format(data_statistics_path)) \n",
    "\n",
    "pprint(data_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrate WordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 76735/76735 [00:02<00:00, 29487.90it/s]\n",
      "Filter uncommon words: 100%|██████████| 90758/90758 [00:00<00:00, 881660.55it/s]\n",
      "INFO:root:token2idx is output to ../data4bilstm/nyt_single/token2idx.json, total token num: 39708\n"
     ]
    }
   ],
   "source": [
    "if config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    all_data = []\n",
    "    for data in list(file_name2data.values()):\n",
    "        all_data.extend(data)\n",
    "        \n",
    "    token2num = {}\n",
    "    for sample in tqdm(all_data, desc = \"Tokenizing\"):\n",
    "        text = sample['text']\n",
    "        for tok in tokenize(text):\n",
    "            token2num[tok] = token2num.get(tok, 0) + 1\n",
    "\n",
    "    token_set = set()\n",
    "    for tok, num in tqdm(token2num.items(), desc = \"Filter uncommon words\"):\n",
    "        if num < 3: # filter words with a frequency of less than 3\n",
    "            continue\n",
    "        token_set.add(tok)\n",
    "        \n",
    "    token2idx = {tok:idx + 2 for idx, tok in enumerate(sorted(token_set))}\n",
    "    token2idx[\"<PAD>\"] = 0\n",
    "    token2idx[\"<UNK>\"] = 1\n",
    "#     idx2token = {idx:tok for tok, idx in token2idx.items()}\n",
    "    \n",
    "    dict_path = os.path.join(data_out_dir, \"token2idx.json\")\n",
    "    json.dump(token2idx, open(dict_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "    logging.info(\"token2idx is output to {}, total token num: {}\".format(dict_path, len(token2idx))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
